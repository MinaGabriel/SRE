{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2348107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers.utils import logging as hf_logging\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import subprocess\n",
    "import unicodedata\n",
    "\n",
    "# Scientific and numerical computing\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Hugging Face and NLP libraries\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Pyserini (for sparse retrieval)\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "from pyserini.index.lucene import LuceneIndexReader\n",
    "\n",
    "# Utilities\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "import string\n",
    "import ast\n",
    "from typing import Union, List, Set\n",
    "\n",
    "hf_logging.set_verbosity_error()  # mute HF warnings\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=False,\n",
    ")\n",
    "\n",
    "print(f\"Loading {model_name} in 8-bit on cuda:0 (RTX 3090)...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map={\"\": 0},          # <<< use the Ampere GPU\n",
    "    attn_implementation=\"eager\", # extra safety: disable FlashAttention usage path\n",
    ")\n",
    "\n",
    "# Make sure we have a pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Device map: {model.hf_device_map}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17916317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_candidates_prompt(question: str, snippets: list) -> str:\n",
    "    \"\"\"\n",
    "    Build a prompt that asks the model to return ALL possible answers\n",
    "    as a Python list of strings, based only on the snippets.\n",
    "    \"\"\"\n",
    "    # Turn snippets into a simple bullet list block\n",
    "    snippets_text = \"\"\n",
    "    for snippet in snippets:\n",
    "        contents = snippet.get(\"contents\", \"\").strip()\n",
    "        snippets_text += f\"- {contents}\\n\"\n",
    "\n",
    "    return (\n",
    "        \"You are a question-answer extraction module.\\n\"\n",
    "        \"You are given a question and several text snippets.\\n\"\n",
    "        \"ASSUME at least one snippet is relevant to the question.\\n\"\n",
    "        \"Use ONLY the snippets (ignore all outside knowledge).\\n\"\n",
    "        \"\\n\"\n",
    "        \"TASK:\\n\"\n",
    "        \"- Extract ALL possible answers that directly answer the question.\\n\"\n",
    "        \"- Answers must be short (1–3 words), normalized (lowercase), and taken from snippet wording.\\n\"\n",
    "        \"- If multiple labels appear (e.g. \\\"journalist, broadcaster, writer\\\"), return them all.\\n\"\n",
    "        \"- If the question refers to a person, extract all occupations/roles that apply.\\n\"\n",
    "        \"- The output MUST be a Python string array (list[str]).\\n\"\n",
    "        \"- If no answers exist in snippets, return: [\\\"unknown\\\"].\\n\"\n",
    "        \"\\n\"\n",
    "        \"OUTPUT FORMAT (EXTREMELY IMPORTANT):\\n\"\n",
    "        \"Return ONLY a Python-style list of strings. No explanations.\\n\"\n",
    "        \"Example valid outputs:\\n\"\n",
    "        \"  [\\\"politician\\\", \\\"soldier\\\"]\\n\"\n",
    "        \"  [\\\"journalist\\\"]\\n\"\n",
    "        \"  [\\\"actor\\\", \\\"producer\\\", \\\"director\\\"]\\n\"\n",
    "        \"  [\\\"unknown\\\"]\\n\"\n",
    "        \"\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        \"Snippets:\\n\"\n",
    "        f\"{snippets_text.strip()}\\n\"\n",
    "        \"\\n\"\n",
    "        \"Return a Python list of strings representing ALL valid answers:\"\n",
    "    )\n",
    "import ast\n",
    "import torch\n",
    "\n",
    "def get_candidates(question: str, snippets: list) -> list[str]:\n",
    "    \"\"\"\n",
    "    Call the LLM to extract all candidate answers.\n",
    "    Returns a Python list of strings: [\"politician\", \"soldier\", ...].\n",
    "    \"\"\"\n",
    "    # Build prompt\n",
    "    prompt = build_candidates_prompt(question, snippets)\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Optional: inspect prompt token length (uncomment if needed)\n",
    "    # print(\"Prompt tokens:\", inputs[\"input_ids\"].shape[-1])\n",
    "\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,\n",
    "            do_sample=False,\n",
    "            temperature=0.0,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "    # Decode only the generated part (after the prompt)\n",
    "    input_len = inputs[\"input_ids\"].shape[-1]\n",
    "    new_ids = outputs[0][input_len:]\n",
    "    response_text = tokenizer.decode(new_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    # Take the first non-empty line as the model's \"list\" output\n",
    "    first_line = \"\"\n",
    "    for line in response_text.splitlines():\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            first_line = line\n",
    "            break\n",
    "\n",
    "    if not first_line:\n",
    "        return [\"unknown\"]\n",
    "\n",
    "    # Try to parse as a Python list of strings\n",
    "    try:\n",
    "        parsed = ast.literal_eval(first_line)\n",
    "        if isinstance(parsed, list):\n",
    "            cleaned = []\n",
    "            for item in parsed:\n",
    "                if isinstance(item, str):\n",
    "                    label = item.strip()\n",
    "                    if label:\n",
    "                        cleaned.append(label)\n",
    "            if cleaned:\n",
    "                return cleaned\n",
    "    except Exception:\n",
    "        # If parsing fails, fall back below\n",
    "        pass\n",
    "\n",
    "    # Fallback: treat the whole line as a single answer string\n",
    "    return [first_line.strip().strip('\"').strip(\"'\")] or [\"unknown\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd7b3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model_loader import ModelLoader\n",
    "\n",
    "\n",
    "# Step 1: Load models once\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "loader = ModelLoader(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d4abed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_yes_sentences(yes_sents):\n",
    "    \"\"\"\n",
    "    Example ranking function – adjust if you already have one.\n",
    "    \"\"\"\n",
    "    return sorted(\n",
    "        yes_sents,\n",
    "        key=lambda s: (\n",
    "            -s[\"p_yes\"],        # higher p_yes first\n",
    "            -s[\"dense_score\"],  # then higher dense score\n",
    "             s.get(\"dense_rank\", 1e9),\n",
    "             s.get(\"bm25_rank\", 1e9),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def compose_context_adaptive( scored_results: list, primary_thr: float = 0.8, fallback_thr: float = 0.7, max_sentences: int = 12, ) -> list:\n",
    "    \"\"\"\n",
    "    From scored_results, build a list of snippet dicts:\n",
    "        [{\"id\": 1, \"contents\": \"...\"},\n",
    "         {\"id\": 2, \"contents\": \"...\"},\n",
    "         ...]\n",
    "    - Filters by p_yes (primary + fallback thresholds).\n",
    "    - Falls back to top-K by dense_score if needed.\n",
    "    - Deduplicates by sentence text.\n",
    "    \"\"\"\n",
    "    if not scored_results:\n",
    "        return []\n",
    "\n",
    "    # 1) Filter by primary threshold\n",
    "    yes_sents = [s for s in scored_results if s[\"p_yes\"] >= primary_thr]\n",
    "\n",
    "    # 2) Fallback threshold if nothing passes primary\n",
    "    if not yes_sents:\n",
    "        yes_sents = [s for s in scored_results if s[\"p_yes\"] >= fallback_thr]\n",
    "\n",
    "    # 3) Final fallback: top-K by dense score if still empty\n",
    "    if not yes_sents:\n",
    "        yes_sents = sorted(\n",
    "            scored_results,\n",
    "            key=lambda s: -s[\"dense_score\"],\n",
    "        )[:max_sentences]\n",
    "\n",
    "    # 4) Rank sentences\n",
    "    ranked = rank_yes_sentences(yes_sents)\n",
    "\n",
    "    # 5) Deduplicate by sentence text, build snippets\n",
    "    seen_sentences = set()\n",
    "    snippets = []\n",
    "    for s in ranked:\n",
    "        sent = s[\"sentence\"].strip()\n",
    "        if not sent:\n",
    "            continue\n",
    "        if sent in seen_sentences:\n",
    "            continue\n",
    "\n",
    "        seen_sentences.add(sent)\n",
    "\n",
    "        snippets.append(\n",
    "            {\n",
    "                \"id\": len(snippets) + 1,   # simple incremental ID\n",
    "                \"contents\": sent,\n",
    "                # you can keep extra metadata if you ever need:\n",
    "                # \"p_yes\": s[\"p_yes\"],\n",
    "                # \"dense_score\": s[\"dense_score\"],\n",
    "                # \"bm25_rank\": s[\"bm25_rank\"],\n",
    "                # \"docid\": s[\"docid\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if len(snippets) >= max_sentences:\n",
    "            break\n",
    "\n",
    "    return snippets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d2e661",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"akariasai/PopQA\")\n",
    "from sre_rag import SreRAG\n",
    "sreg = SreRAG(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b70a63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import re\n",
    "import string\n",
    "import ast\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Union, List, Set\n",
    "\n",
    "# 1) Configure logger to write to file only, simple format (no timestamps/levels)\n",
    "logger = logging.getLogger(\"qa_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Remove existing handlers (if any)\n",
    "for h in list(logger.handlers):\n",
    "    logger.removeHandler(h)\n",
    "\n",
    "file_handler = logging.FileHandler(\"qa_results.log\", mode=\"w\", encoding=\"utf-8\")\n",
    "formatter = logging.Formatter(\"%(message)s\")  # only message\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Prevent propagation to root logger (nothing goes to console)\n",
    "logger.propagate = False\n",
    "\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = s.lower()\n",
    "    s = re.sub(rf\"[{re.escape(string.punctuation)}]\", \" \", s)\n",
    "    s = re.sub(r\"\\b(a|an|the)\\b\", \" \", s)\n",
    "    return \" \".join(s.split())\n",
    "\n",
    "def popqa_match(pred: Union[str, List[str]],\n",
    "                gold_list: Union[str, List[str]]) -> bool:\n",
    "    \"\"\"\n",
    "    Keyword-overlap match between prediction(s) and PopQA possible_answers.\n",
    "\n",
    "    pred:\n",
    "        - a single string answer, or\n",
    "        - a list of answer strings (e.g., [\"journalist\", \"broadcaster\"])\n",
    "    gold_list:\n",
    "        - a Python list of strings, or\n",
    "        - a string representation of a list (e.g., '[\"journalist\",\"journo\"]')\n",
    "    \"\"\"\n",
    "    # Handle missing inputs\n",
    "    if not pred or not gold_list:\n",
    "        return False\n",
    "\n",
    "    # Normalize pred into a Python list of strings\n",
    "    if isinstance(pred, str):\n",
    "        pred_list = [pred]\n",
    "    elif isinstance(pred, (list, tuple)):\n",
    "        pred_list = [p for p in pred if isinstance(p, str)]\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    if not pred_list:\n",
    "        return False\n",
    "\n",
    "    # Normalize gold_list into a Python list of strings\n",
    "    if isinstance(gold_list, str):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(gold_list)\n",
    "            gold = parsed if isinstance(parsed, list) else [gold_list]\n",
    "        except Exception:\n",
    "            gold = [gold_list]\n",
    "    elif isinstance(gold_list, (list, tuple)):\n",
    "        gold = list(gold_list)\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    if not gold:\n",
    "        return False\n",
    "\n",
    "    # Precompute normalized gold tokens\n",
    "    gold_tokens_list: List[Set[str]] = []\n",
    "    for g in gold:\n",
    "        g_norm = normalize_text(g)\n",
    "        if not g_norm:\n",
    "            continue\n",
    "        gold_tokens_list.append(set(g_norm.split()))\n",
    "\n",
    "    if not gold_tokens_list:\n",
    "        return False\n",
    "\n",
    "    # Check each predicted answer against all gold aliases\n",
    "    for p in pred_list:\n",
    "        p_norm = normalize_text(p)\n",
    "        if not p_norm:\n",
    "            continue\n",
    "        pred_tokens: Set[str] = set(p_norm.split())\n",
    "\n",
    "        for gold_tokens in gold_tokens_list:\n",
    "            if pred_tokens & gold_tokens:\n",
    "                return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "# 2) Main evaluation loop with tqdm, logging to file only\n",
    "ems = []\n",
    "BM25_TOP_K = 100\n",
    "FUSION_RRF_TOP_K = 3\n",
    "\n",
    "for s in tqdm(dataset[\"test\"].select(range(25)), desc=\"Evaluating\"):\n",
    "    question = s[\"question\"]\n",
    "    gold = s.get(\"possible_answers\", [])\n",
    "\n",
    "    bm25 = sreg.bm25_retrieve_and_rank(question, bm25_k=100)\n",
    "    dense = sreg.dense_retrieve_and_rank(question, bm25)\n",
    "\n",
    "    # NEW: fuse BM25 and dense before classification\n",
    "    fused_docs = sreg.fuse_bm25_and_dense(dense, top_k=3)  # keep top 50 docs\n",
    "\n",
    "    # Now score sentences only inside those fused docs\n",
    "    scored = sreg.score_candidates(question, fused_docs) # yes/no threshold\n",
    "\n",
    "    snippets = compose_context_adaptive(scored)\n",
    "    pred = get_candidates(question, snippets)\n",
    "\n",
    "\n",
    "    # Compute EM\n",
    "    em = 1.0 if popqa_match(pred, gold) else 0.0\n",
    "    ems.append(em)\n",
    "\n",
    "    # ---- Logging block (goes ONLY to qa_results.log) ----\n",
    "    logger.info(f\"Q: {question}\")\n",
    "    logger.info(\"Snippets:\")\n",
    "    for sn in snippets:\n",
    "        # handle both dict and plain string snippets\n",
    "        if isinstance(sn, dict):\n",
    "            text = sn.get(\"contents\", \"\").strip()\n",
    "        else:\n",
    "            text = str(sn).strip()\n",
    "        logger.info(f\"- {text}\")\n",
    "    logger.info(\"Pred: %s\", json.dumps(pred, ensure_ascii=False))\n",
    "    logger.info(f\"Gold: {gold}\")\n",
    "    logger.info(f\"EM: {em}\")\n",
    "    logger.info(\"-\" * 40)\n",
    "\n",
    "# Overall EM\n",
    "overall = 100.0 * sum(ems) / len(ems) if ems else 0.0\n",
    "logger.info(f\"Overall EM {overall:.2f} on {len(ems)} examples\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nars_ir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
